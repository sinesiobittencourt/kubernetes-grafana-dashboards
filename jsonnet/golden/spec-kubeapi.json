{
   "grafana": {
      "templates_custom": {
         "api_percentile": {
            "default": "90",
            "hide": "",
            "values": "50, 90, 99"
         },
         "availability_span": {
            "default": "7d",
            "hide": "",
            "values": "10m,1h,1d,7d,30d,90d"
         },
         "verb_excl": {
            "default": "(CONNECT|WATCH|PROXY)",
            "hide": "variable",
            "values": "(CONNECT|WATCH|PROXY)"
         }
      }
   },
   "metrics": {
      "kube_api": {
         "alerts": {
            "blackbox": {
               "annotations": {
                  "description": "Issue: Kube API is not responding 200s from blackbox.monitoring\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPIUnHealthy\n",
                  "summary": "Kube API is unhealthy"
               },
               "expr": "probe_success{provider=\"kubernetes\"} == 0\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeAPIUnHealthy"
            },
            "error_ratio": {
               "annotations": {
                  "description": "Issue: Kube API Error ratio on {{ $labels.instance }} is above 0.01: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPIErrorRatioHigh\n",
                  "summary": "Kube API 500s ratio is High"
               },
               "expr": "sum by (instance)(\n  rate(apiserver_request_count{verb!~\"(CONNECT|WATCH|PROXY)\", code=~\"5..\"}[5m])\n) /\nsum by (instance)(\n  rate(apiserver_request_count[5m])\n) > 0.01\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeAPIErrorRatioHigh"
            },
            "latency": {
               "annotations": {
                  "description": "Issue: Kube API Latency on {{ $labels.instance }} is above 200 ms: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeAPILatencyHigh\n",
                  "summary": "Kube API Latency is High"
               },
               "expr": "histogram_quantile (\n  0.90,\n  sum by (le, instance)(\n    rate(apiserver_request_latencies_bucket{verb!~\"(CONNECT|WATCH|PROXY)\"}[5m])\n  )\n) / 1e3 > 200\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeAPILatencyHigh"
            }
         },
         "api_percentile": "90",
         "error_ratio_threshold": 0.01,
         "graphs": {
            "availability_1": {
               "format": "percentunit",
               "formula": "sum_over_time(kubernetes::job:slo_kube_api_ok[$availability_span]) / sum_over_time(kubernetes::job:slo_kube_api_sample[$availability_span])\n",
               "legend": "{{ job }}",
               "span": 2,
               "threshold": "0.99",
               "title": "SLO: Availaibility over $availability_span",
               "type": "singlestat"
            },
            "availability_2": {
               "formula": "sum_over_time(kubernetes::job:slo_kube_api_ok[10m]) / sum_over_time(kubernetes::job:slo_kube_api_sample[10m])\n",
               "legend": "{{ job }}",
               "span": 10,
               "threshold": "0.99",
               "title": "SLO: Availaibility over 10m"
            },
            "error_ratio": {
               "formula": "sum by (verb, code)(\n  rate(apiserver_request_count{verb!~\"$verb_excl\", code=~\"5..\"}[5m])\n) / ignoring(code) group_left\nsum by (verb)(\n  rate(apiserver_request_count[5m])\n)\n",
               "legend": "{{ verb }} - {{ code }}",
               "threshold": 0.01,
               "title": "API Error ratio 500s/total (except $verb_excl)"
            },
            "latency": {
               "formula": "histogram_quantile (\n  0.$api_percentile,\n  sum by (le, verb)(\n    rate(apiserver_request_latencies_bucket{verb!~\"$verb_excl\"}[5m])\n  )\n) / 1e3 > 0\n",
               "legend": "{{ verb }}",
               "threshold": 200,
               "title": "API $api_percentile-th latency[ms] by verb (except $verb_excl)"
            }
         },
         "latency_threshold": 200,
         "name": "Kube API",
         "rules": {
            "error_ratio_job": {
               "expr": "sum by (job)(\n  kubernetes::job_instance:apiserver_request_errors:ratio_rate5m\n)\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job:apiserver_request_errors:ratio_rate5m"
            },
            "error_ratio_job_instance": {
               "expr": "sum by (job, instance)(\n  rate(apiserver_request_count{verb!~\"(CONNECT|WATCH|PROXY)\", code=~\"5..\"}[5m])\n) /\nsum by (job, instance)(\n  rate(apiserver_request_count[5m])\n)\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job_instance:apiserver_request_errors:ratio_rate5m"
            },
            "latency_job": {
               "expr": "histogram_quantile (\n  0.90,\n  sum by (le, job)(\n    rate(apiserver_request_latencies_bucket{verb!~\"(CONNECT|WATCH|PROXY)\"}[5m])\n  )\n) / 1e3\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job:apiserver_latency:pctl90rate5m"
            },
            "latency_job_instance": {
               "expr": "histogram_quantile (\n  0.90,\n  sum by (le, job, instance)(\n    rate(apiserver_request_latencies_bucket{verb!~\"(CONNECT|WATCH|PROXY)\"}[5m])\n  )\n) / 1e3\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job_instance:apiserver_latency:pctl90rate5m"
            },
            "probe_success": {
               "expr": "sum by()(probe_success{provider=\"kubernetes\", component=\"apiserver\"})\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job:probe_success"
            },
            "slo_ok": {
               "expr": "kubernetes::job:apiserver_request_errors:ratio_rate5m < bool 0.01 * kubernetes::job:apiserver_latency:pctl90rate5m < bool 200\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job:slo_kube_api_ok"
            },
            "slo_sample": {
               "expr": "kubernetes::job:apiserver_request_errors:ratio_rate5m < bool Inf * kubernetes::job:apiserver_latency:pctl90rate5m < bool Inf\n",
               "labels": {
                  "job": "kubernetes_api_slo"
               },
               "record": "kubernetes::job:slo_kube_api_sample"
            }
         },
         "verb_excl": "(CONNECT|WATCH|PROXY)"
      },
      "kube_control_mgr": {
         "alerts": {
            "work_duration": {
               "annotations": {
                  "description": "Issue: Kube Control Manager on {{ $labels.instance }} work duration is above 100: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeControllerWorkDurationHigh\n",
                  "summary": "Kube Control Manager workqueue processing is slow"
               },
               "expr": "sum by (instance)(\n  APIServiceRegistrationController_work_duration{quantile=\"0.9\"}\n) > 100\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeControllerWorkDurationHigh"
            }
         },
         "graphs": {
            "work_duration": {
               "formula": "sum by (instance)(\n  APIServiceRegistrationController_work_duration{quantile=\"0.9\"}\n)\n",
               "legend": "{{ instance }}",
               "threshold": 100,
               "title": "Kube Control Manager work duration"
            }
         },
         "name": "Kube Control Manager",
         "rules": { },
         "work_duration_limit": 100
      },
      "kube_etcd": {
         "alerts": {
            "latency": {
               "annotations": {
                  "description": "Issue: Kube Etcd latency on {{ $labels.instance }} above 1000 ms: {{ $value }}\nPlaybook: https://engineering-handbook.nami.run/sre/runbooks/kubeapi#KubeEtcdLatencyHigh\n",
                  "summary": "Etcd Latency is High"
               },
               "expr": "max by (instance)(\n  etcd_request_latencies_summary{job=\"kubernetes_apiservers\",quantile=\"0.9\"}\n)/ 1e3 > 1000\n",
               "for": "5m",
               "labels": {
                  "notify_to": "slack",
                  "severity": "critical",
                  "slack_channel": "#sre-alerts"
               },
               "name": "KubeEtcdLatencyHigh"
            }
         },
         "etcd_latency_threshold": 1000,
         "graphs": {
            "latency": {
               "formula": "max by (operation, instance)(\n  etcd_request_latencies_summary{job=\"kubernetes_apiservers\",quantile=\"0.9\"}\n)/ 1e3\n",
               "legend": "{{ instance }} - {{ operation }}",
               "threshold": 1000,
               "title": "etcd 90th latency[ms] by (operation, instance)"
            }
         },
         "name": "Kube Etcd",
         "rules": { }
      }
   },
   "prometheus": {
      "alerts_common": {
         "for": "5m",
         "labels": {
            "notify_to": "slack",
            "severity": "critical",
            "slack_channel": "#sre-alerts"
         }
      }
   }
}
